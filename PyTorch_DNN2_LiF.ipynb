{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch で深層学習 2\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "2022/08/01 中山将伸作成<BR>\n",
    "2024/10/25 講習会用に修正（AE追加）<BR>\n",
    "</div>\n",
    "    \n",
    "1) Li-Fデータを使って回帰予測<BR>\n",
    "2) Li-F電気陰性度の分布に対するオートエンコーダー適用例<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Li-Fデータ(sample数1500件超)を用いて回帰分析\n",
    "\n",
    "![image_DNN](./fig/image_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path=str(\"./20161017_LiF_RDF_Voronoi_ME_CE_wMatminer.csv\")\n",
    "df_original=pd.read_csv(path, index_col=0)\n",
    "\n",
    "df = df_original.dropna()\n",
    "\n",
    "drop_columns = ['ID','pretty_formula','Cohesive_Energy','Decomposition_Energy','Decomp_UorS','MEvalues_type1','ME_F/S','ME_F/M/S']\n",
    "df_descriptor = df.drop(drop_columns, axis=1)\n",
    "target_columns = ['MEvalues_type1']\n",
    "df_target = df[target_columns]\n",
    "\n",
    "display(df_descriptor)\n",
    "display(df_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■　保留法　test_size=0.3 として、 X, t -->  train_X, test_X, train_t, test_t  を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_descriptor.values\n",
    "t=df_target.values\n",
    "\n",
    "train_X, test_X, train_t, test_t = train_test_split(X,t,test_size=0.3)\n",
    "print (len(train_X), len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■　numpy -->  Tensor形式変換  (train_X --> train_X のまま)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=torch.Tensor(train_X)\n",
    "train_t=torch.Tensor(train_t)\n",
    "test_X=torch.Tensor(test_X)\n",
    "test_t=torch.Tensor(test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■　modelの定義  1784の記述子を徐々に 1つのノードに集約させる。\n",
    "\n",
    "Tips: <BR>\n",
    "  BatchNorm1d  (データの規格化)<BR>\n",
    "  ReLU  (深層学習ではsigmoidよりもReLUをよく使う)<BR>\n",
    "    \n",
    "![ReLU](./fig/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= nn.Sequential(\n",
    "    nn.Linear(1784,128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128,16),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(16,1),\n",
    ")\n",
    "\n",
    "# model= nn.Sequential(\n",
    "#     nn.Linear(1784,128),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(128,16),\n",
    "#     nn.BatchNorm1d(16),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(16,1),\n",
    "# )\n",
    "\n",
    "# model= nn.Sequential(\n",
    "#     nn.Linear(1784,128),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128,64),\n",
    "#     nn.BatchNorm1d(64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64,16),\n",
    "#     nn.BatchNorm1d(16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(16,1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■　Loss関数の定義。 MSE (mean square error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()   #Loss関数の定義　この場合は MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=0.001)  #model.parameters() 学習させたい変数　　ln は学習率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  #model.parameters() 学習させたい変数　　ln は学習率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■　DNNの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()     # modelをtraining モードにする\n",
    "loss_history=[]      #  lossのステップごとの推移\n",
    "losstest_history=[]\n",
    "for epoch in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()   #optimizerの初期化\n",
    "    train_y=model(train_X)\n",
    "    test_y=model(test_X)\n",
    "    loss=loss_fn(train_y,train_t)   #Loss関数の計算\n",
    "    losstest=loss_fn(test_y,test_t)\n",
    "    loss_history.append(float(loss))\n",
    "    losstest_history.append(float(losstest))\n",
    "    loss.backward()   #傾きを計算\n",
    "    optimizer.step()   #更新処理実施\n",
    "\n",
    "plt.plot(loss_history)  #blue\n",
    "plt.plot(losstest_history) #orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (loss_fn(train_y,train_t))\n",
    "plt.plot(train_y.detach().numpy(),train_t.detach().numpy(),'x')\n",
    "test_y=model(test_X)\n",
    "plt.plot(test_y.detach().numpy(),test_t.detach().numpy(),'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.オートエンコーダーをつくる\n",
    "\n",
    "\n",
    "![Autoencoder](./fig/image_AE.png)<BR>\n",
    "注）　optimizerを二つに分けているところは自信がありません。（通常は classを使って、モデルを作ります）<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xをつくる（電気陰性度 EN_1～EN_50のみを対象にする)\n",
    "\n",
    "df_select=df_descriptor.iloc[:,0:50]\n",
    "display(df_select)\n",
    "\n",
    "X=df_select.values #pandas > numpyにする\n",
    "train_X, test_X = train_test_split(X,test_size=0.3)  #保留法　\n",
    "print (len(train_X), len(test_X))\n",
    "train_X=torch.Tensor(train_X)\n",
    "test_X=torch.Tensor(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encode= nn.Sequential(\n",
    "    nn.Linear(50,32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,16),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,2),\n",
    ")\n",
    "\n",
    "model_decode= nn.Sequential(\n",
    "    nn.Linear(2,16),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,50)\n",
    ")\n",
    "\n",
    "optimizer1 = optim.Adam(model_encode.parameters(), lr=0.001)  #model.parameters() 学習させたい変数　　ln は学習率\n",
    "optimizer2 = optim.Adam(model_decode.parameters(), lr=0.001)  #model.parameters() 学習させたい変数　　ln は学習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=[]\n",
    "losstest_history=[]\n",
    "\n",
    "for epoch in tqdm(range(3000)):\n",
    "    model_encode.train()\n",
    "    model_decode.train()\n",
    "    optimizer1.zero_grad()   #optimizerの初期化\n",
    "    optimizer2.zero_grad()   #optimizerの初期化\n",
    "\n",
    "    y=model_encode(train_X)\n",
    "    rX=model_decode(y)\n",
    "    ty=model_encode(test_X)\n",
    "    rtX=model_decode(ty)\n",
    "\n",
    "    loss=loss_fn(train_X,rX)\n",
    "    loss.backward()   #傾きを計算\n",
    "    optimizer1.step()   #更新処理実施\n",
    "    optimizer2.step()   #更新処理実施\n",
    "    \n",
    "    loss_history.append(float(loss))\n",
    "    losstest=loss_fn(rtX,test_X)\n",
    "    losstest_history.append(float(losstest))\n",
    "\n",
    "plt.plot(loss_history)  #blue\n",
    "plt.plot(losstest_history) #orange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンコーダで特徴抽出\n",
    "X = train_X\n",
    "tX = test_X\n",
    "\n",
    "y = model_encode(X)\n",
    "ty = model_encode(tX)\n",
    "\n",
    "# デコーダで再構成\n",
    "rX = model_decode(y)\n",
    "rtX = model_decode(ty)\n",
    "\n",
    "# 損失を表示\n",
    "print(\"LOSS(train):\", loss_fn(rX, X).item())\n",
    "print(\"LOSS(test):\", loss_fn(rtX, tX).item())\n",
    "\n",
    "\n",
    "# データの形状を確認\n",
    "print(\"Original X shape:\", X.shape)\n",
    "print(\"Reconstructed rX shape:\", rX.shape)\n",
    "\n",
    "# 2次元または1次元データに変換してプロット\n",
    "# ここでは最初のデータポイントのみをプロットする例を示しています\n",
    "plt.figure()\n",
    "plt.plot(X.detach().reshape(-1).numpy(),rX.detach().reshape(-1).numpy(),  '+')  # 最初のデータポイントをプロット\n",
    "plt.plot(tX.detach().reshape(-1).numpy(),rtX.detach().reshape(-1).numpy(),  '+')  # 最初のデータポイントをプロット\n",
    "plt.ylabel(\"Reconstructed X\")\n",
    "plt.xlabel(\"Original X\")\n",
    "plt.title(\"Reconstructed vs Original Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "plt.plot(y.detach().numpy()[:,0],y.detach().numpy()[:,1],'x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
